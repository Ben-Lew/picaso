{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6da004-2fb1-47fe-b934-0e2ed17c2369",
   "metadata": {},
   "source": [
    "# Retrieval 101 \n",
    "\n",
    "## PICASO Retrieval Philosophy: How is this \"retrieval\" code different from others? \n",
    "\n",
    "A \"retrieval\" is a word we use to describe the coupling of a forward model (e.g. picaso) with a statistical sampler (e.g. ultranest, dynesty, pymultinest) in order to conduct spectral inference -- getting constraint intervals for physical parameters. Check out this [great Bayesian workflow tutorial](https://johannesbuchner.github.io/UltraNest/example-sine-bayesian-workflow.html) which shows the simple exercise of setting up a \"retrieval\" for sine model with gaussian measurement errors. An atmospheric retrieval is no different from this example as we will show below. The major difference being of course, the use of an atmospheric model, in place of the simple sine curve. \n",
    "\n",
    "The main difference between PICASO and other codes is that PICASO's retrieval code is much more stripped down. You will see much more similarity between the tutorials that follow below, and the tutorials on the websites of the statistical samplers themselves. \n",
    "\n",
    "**Cons:** if you want more of a \"plug and play\" appraoch to retrieving atmospheric properties this is probably not the code for you and something like [POSEIDON](https://poseidon-retrievals.readthedocs.io/en/latest/content/notebooks/retrieval_basic.html#Creating-a-Retrieval-Model) might be a quicker setup for you. **Pros:** if you want to learn how to write your own parameterizations and retrieval setups then this route might be good for you. We will walk you through setting up several flavors of retrievals (grid only, grids+post processing, fully free retrivals). After you learn the basics, we will introduce some of our autobuild tools to writing scripts. \n",
    "\n",
    "## Making sense of grid fits, gridtrievals, free retrievals\n",
    "\n",
    "The graphic below should help clarify the difference between these different spectral inference schemes. In order to create a spectra that we can compare to data we need, as input, planet/stellar parameters, a pressure-temperature profile, and set of chemical abundances as a function of pressure. The major difference between these three setups is in how you create the input. We can either use a set of radiative-convective models to pre-compute pressure-temperature and chemistry. Or, we can setup simple parameterizations (e.g. could be as simple as an isothermal lines) to build these inputs. The \"magic sauce\" of retrievals is all in how we build these parameterizations. \n",
    "\n",
    "![image](retrievalfig/retrievalfig.001.jpeg)\n",
    "\n",
    "\n",
    "## Retrieval Tutorial Overview\n",
    "\n",
    "Our full retrieval tutorial takes you through: \n",
    "\n",
    "1. The basics of our retrievals workflow: the data, the model set, the prior set, the likelihood function (**this tutorial**)\n",
    "2. The basics of a retrieval analysis: corner plots, Bayesian evidence, 1-3 sigma banded spectra (**this tutorial**)\n",
    "3. Implemenation: Grid Fitting -- Using only pre-computed grids to get best fit grid values\n",
    "4. Implementation: Grid-trieval -- Using a pre-computed grid and post-processing values such as clouds, added chemistry, etc\n",
    "5. Implementation: Free retrieval -- Building parameterizations \n",
    "\n",
    "All of these retrieval tutorials are done using the [WASP-17 b Grant et al data.](https://zenodo.org/records/8360121) and associated pre-computed PICASO model grid. \n",
    "\n",
    "# The Basics of Retrieval Workflow\n",
    "\n",
    "All PICASO retrieval scripts will include at least: \n",
    "\n",
    "1. function `get_data` : a function to get your spectral data\n",
    "2. class `param_set` : a class that contains the set of free parameters you want to retrieve\n",
    "3. class `guesses_set` : an option class that will help you test your models \n",
    "4. class `model_set` : a class that contains the set of models you want to test\n",
    "5. class `prior_set` : a class that define the prior\n",
    "6. function `loglikelihood` : a function that calls your model and computes the likelihood \n",
    "7. statistical sampling run script : code that executes the sampler (e.g. ultranest)\n",
    "\n",
    "Below we break these down with the simple function of fitting a line to a spectrum. If you understand the basics here of fitting some model to a spectrum, and analyzing the results, you will be well positioned to move onto the next tutorials where we swap the simple line function for a picaso function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669e156-5b15-47ca-b25a-33b4a9385397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import picaso.justdoit as jdi\n",
    "import picaso.analyze as lyz\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a1039-5c45-4951-94c7-b07043ebbb71",
   "metadata": {},
   "source": [
    "## Step 1) Develop function to get data\n",
    "\n",
    "Let's create a function to pull our data where all we need to do is declare who the data is from and it pulls it for us automatically.\n",
    "\n",
    "Note: this format is only a recommendation and you can change any part of this to fit your needs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca69d4b-d302-453c-acbc-7e0c930dc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    \"\"\"\n",
    "    Create a function to process your data in any way you see fit.\n",
    "    Here we are using the ExoTiC-MIRI data \n",
    "    https://zenodo.org/records/8360121/files/ExoTiC-MIRI.zip?download=1\n",
    "    But no need to download it.\n",
    "\n",
    "    Checklist\n",
    "    ---------\n",
    "    - your function returns a spectrum that will be in the same units as your picaso model (e.g. rp/rs^2, erg/s/cm/cm or other) \n",
    "    - your function retuns a spectrum that is in ascending order of wavenumber \n",
    "    - your function returns a dictionary where the key specifies the instrument name (in the event there are multiple)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: \n",
    "        dictionary key: wavenumber (ascneding), flux or transit depth, and error.\n",
    "        e.g. {'MIRI LRS':[wavenumber, transit depth, transit depth error], 'NIRSpec G395H':[wavenumber, transit depth, transit depth error]}\n",
    "    \"\"\"\n",
    "    dat = xr.load_dataset(jdi.w17_data())\n",
    "    #build nice dataframe so we can easily \n",
    "    final = jdi.pd.DataFrame(dict(wlgrid_center=dat.coords['central_wavelength'].values,\n",
    "                transit_depth=dat.data_vars['transit_depth'].values,\n",
    "                transit_depth_error=dat.data_vars['transit_depth_error'].values))\n",
    "\n",
    "    #create a wavenumber grid \n",
    "    final['wavenumber'] = 1e4/final['wlgrid_center']\n",
    "\n",
    "    #always ensure we are ordered correctly\n",
    "    final = final.sort_values(by='wavenumber').reset_index(drop=True)\n",
    "\n",
    "    #return a nice dictionary with the info we need \n",
    "    returns = {'MIRI_LRS': [final['wavenumber'].values, \n",
    "             final['transit_depth'].values  ,final['transit_depth_error'].values]   }\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa11547-dd36-4add-92a1-f387fbb98244",
   "metadata": {},
   "source": [
    "## Step 2) Define Free Parameters\n",
    "\n",
    "In what follows we build three classes that will (in the future) help us keep track of all the models we test for a single planet case. For this simple tutorial let's just do the simplest thing and retrieve the simplest model possible: \n",
    "\n",
    "$$y = mx + b$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13e3cd-6ca3-4bf5-841c-ecba467f5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "class param_set:\n",
    "    \"\"\"\n",
    "    This is for book keeping what parameters you have run in each retrieval.\n",
    "    It helps if you keep variables uniform.\n",
    "    \n",
    "    Checklist\n",
    "    ---------\n",
    "    - Make sure that the order of variables here match how you are unpacking your cube in the model_set class and prior_set\n",
    "    - Make sure that the variable names here match the function names in model_set and prior_set\n",
    "    \"\"\"\n",
    "    line=['m','b','log_err_inf'] \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b3b4-5320-406b-9581-562dcc5cacb1",
   "metadata": {},
   "source": [
    "## Step 3) Define Initial Guesses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f686b-5bd2-4847-a5b6-581c007afeca",
   "metadata": {},
   "source": [
    "In testing, it is very useful to check that it is grabbing the right parameters before doing a full analysis. Also if you choose to use emcee instead of MultiNest these can serve as your starting values for your chain. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a881c5-15e8-4f9d-a1fe-5881d002501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class guesses_set: \n",
    "    \"\"\"\n",
    "    Optional! \n",
    "\n",
    "    Tips\n",
    "    ----\n",
    "    - Usually you might have some guess (however incorrect) of what the answer might be. You can use this in the testing phase!\n",
    "    \"\"\"\n",
    "    line=[0,0.016633,-1,1] #here I am guessing a zero slope, and the measured transit depth reported from exo.mast, and a small error inflation term"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597fc70-bdfe-49fc-bc4f-2c07f9c5644c",
   "metadata": {},
   "source": [
    "## Step 4) Define Model Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d36dd-ad98-44f9-9354-5e18ccf9f42b",
   "metadata": {},
   "source": [
    "Here, we are defining the full model. This is essentially prepping and making it easy to digest for Ultranest's `cube` usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0214-5a97-4ec7-a3ec-2ba95f333f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_set:\n",
    "    \"\"\"\n",
    "    This is your full model set. It will include all the functions you want to test\n",
    "    for a particular data set.\n",
    "\n",
    "    Tips\n",
    "    ----\n",
    "    - if you keep the structure of all your returns identically you will thank yourself later. \n",
    "      For exmaple, below I always return x,y,dict of instrument offsets,dict of error inflation, if exists\n",
    "\n",
    "    Checklist\n",
    "    ---------\n",
    "    - unpacking the cube should unpack the parameters you have set in your param_set class. I like to use \n",
    "    list indexing with strings so I dont have to rely on remembering a specific order\n",
    "    \"\"\"     \n",
    "    def line(cube): \n",
    "        wno_grid = np.linspace(600,3000,int(1e4)) #in the future this will be defined by the picaso opacity db\n",
    "        m = cube[param_set.line.index('m')] \n",
    "        b = cube[param_set.line.index('b')] \n",
    "        err_inf = {'MIRI_LRS':10**cube[param_set.line.index('log_err_inf')] }\n",
    "        y = m*wno_grid + b \n",
    "        offsets = {} #I like to keep the returns of all my model sets the same \n",
    "        return wno_grid,y,offsets,err_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4e125-135f-4bfd-a60f-a744febc8c2d",
   "metadata": {},
   "source": [
    "## Step 5) Define Prior Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e3ae0-e80e-4e34-acea-0a58fa6263f6",
   "metadata": {},
   "source": [
    "Finally, we are storing all the priors for Ultranest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74043db2-4dec-4622-bded-86d1aa36502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_set:\n",
    "    \"\"\"\n",
    "    Store all your priors. You should have the same exact function names in here as\n",
    "    you do in model_set and param_set\n",
    "\n",
    "    Checklist\n",
    "    ---------\n",
    "    - Make sure the order of the unpacked cube follows the unpacking in your model \n",
    "      set and in your parameter set. \n",
    "    \"\"\"   \n",
    "    def line(cube):#,ndim, nparams):\n",
    "        params = cube.copy()\n",
    "        #slope min max\n",
    "        min = -1e-5\n",
    "        max = 1e-5\n",
    "        i=0;params[i] = min + (max-min)*params[i];i+=1\n",
    "        #intercept min max\n",
    "        min = 0.015\n",
    "        max = 0.02\n",
    "        params[i] = min + (max-min)*params[i];i+=1\n",
    "        #log err inflation min max \n",
    "        min = -10\n",
    "        max = 3\n",
    "        params[i] = min + (max-min)*params[i];i+=1\n",
    "        return params                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45d81ad8-68a6-47f4-affa-b503f6c1c7b7",
   "metadata": {},
   "source": [
    "## Step 6) Define Likelihood Function\n",
    "\n",
    "Most likelihood functions have the same form (see for example the formalism in [emcee](https://emcee.readthedocs.io/en/stable/tutorials/line/#maximum-likelihood-estimation)). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff1179d5-63c5-49c6-82af-e9fa7e6f7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(cube):\n",
    "    \"\"\"\n",
    "    Log_likelihood function that ultimately is given to the sampler\n",
    "    Note if you keep to our same formats you will not have to change this code move \n",
    "\n",
    "    Tips\n",
    "    ----\n",
    "    - Remember how we put our data dict, error inflation, and offsets all in dictionary format? Now we can utilize that \n",
    "    functionality if we properly named them all with the right keys! \n",
    "\n",
    "    Checklist\n",
    "    --------- \n",
    "    - ensure that error inflation and offsets are incorporated in the way that suits your problem \n",
    "    - note there are many different ways to incorporate error inflation! this is just one example \n",
    "    \"\"\"\n",
    "    #compute model spectra\n",
    "    resultx,resulty,offset_all,err_inf_all = MODEL(cube) # we will define MODEL below \n",
    "\n",
    "    #initiate the four terms we willn eed for the likelihood\n",
    "    ydat_all=[];ymod_all=[];sigma_all=[];extra_term_all=[];\n",
    "\n",
    "    #loop through data (if multiple instruments, add offsets if present, add err inflation if present)\n",
    "    for ikey in DATA_DICT.keys(): #we will also define DATA_DICT below\n",
    "        xdata,ydata,edata = DATA_DICT[ikey]\n",
    "        xbin_model , y_model = jdi.mean_regrid(resultx, resulty, newx=xdata)#remember we put everything already sorted on wavenumber\n",
    "\n",
    "        #add offsets if they exist\n",
    "        offset = offset_all.get(ikey,0) #if offset for that instrument doesnt exist, return 0\n",
    "        ydata = ydata+offset \n",
    "\n",
    "        #add error inflation if they exist\n",
    "        err_inf = err_inf_all.get(ikey,0) #if err inf term for that instrument doesnt exist, return 0\n",
    "        sigma = edata**2 + (err_inf)**2 #there are multiple ways to do this, here just adding in an extra noise term\n",
    "        if err_inf !=0: \n",
    "            #see formalism here for example https://emcee.readthedocs.io/en/stable/tutorials/line/#maximum-likelihood-estimation\n",
    "            extra_term = np.log(2*np.pi*sigma)\n",
    "        else: \n",
    "            extra_term=sigma*0\n",
    "\n",
    "        ydat_all.append(ydata);ymod_all.append(y_model);sigma_all.append(sigma);extra_term_all.append(extra_term); \n",
    "\n",
    "    ymod_all = np.concatenate(ymod_all)    \n",
    "    ydat_all = np.concatenate(ydat_all)    \n",
    "    sigma_all = np.concatenate(sigma_all)  \n",
    "    extra_term_all = np.concatenate(extra_term_all)\n",
    "\n",
    "    #compute likelihood\n",
    "    loglike = -0.5*np.sum((ydat_all-ymod_all)**2/sigma_all + extra_term_all)\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e521a2a2-444c-417f-beb5-378ef5c74569",
   "metadata": {},
   "source": [
    "## Step 7) Check models, likelihoods, priors! \n",
    "\n",
    "Do not undersestimate the importance of this step before moving forward. You always want to ensure that your model is returning sensible values before you jump into running your sampler. \n",
    "\n",
    "Ask yourself: \n",
    "\n",
    "1. Do your random tests about your prior approximately go through the data\n",
    "2. Do they seem skewed? If so, maybe you need to adjust your prior?\n",
    "3. Do the likelihood values track with the models? E.g. lower likelihoods for bad models, higher likelihoods for good models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7902f2c-4644-49a1-8df1-ce93bd59dc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can easity grab all the important pieces now that they are neatly stored in a class structure \n",
    "DATA_DICT = get_data()\n",
    "PARAMS = getattr(param_set,'line')\n",
    "MODEL = getattr(model_set,'line')\n",
    "PRIOR = getattr(prior_set,'line')\n",
    "GUESS = getattr(guesses_set,'line')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803b65ed-8c3f-4c96-b009-f707517086b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.figure()\n",
    "#lets plot the data \n",
    "for ikey in DATA_DICT.keys(): \n",
    "    plt.errorbar(x=DATA_DICT[ikey][0], y=DATA_DICT[ikey][1], yerr=DATA_DICT[ikey][2], marker='o', ls=' ',label='Grant data')\n",
    "\n",
    "ntests = 10 #lets do 10 random tests \n",
    "for i in range(ntests): \n",
    "    cube = np.random.uniform(size=len(PARAMS))\n",
    "    params_evaluations = PRIOR(cube)\n",
    "    x,y,off,err = MODEL(params_evaluations)\n",
    "    loglike = loglikelihood(params_evaluations)\n",
    "    plt.plot(x,y,label=str(int(loglike)))\n",
    "\n",
    "guessx,guessy,off,err = MODEL(GUESS)\n",
    "guess_log = loglikelihood(GUESS)\n",
    "plt.plot(guessx,guessy,color='black',label='guess '+ str(int(guess_log)))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41e137fd-49b3-41b3-8ec5-ed76f4b7bd8b",
   "metadata": {},
   "source": [
    "Looks pretty good! We might be tiny bit skewed toward higher y intercept values but overall things look good. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4736935f-4662-403c-b722-713d72952108",
   "metadata": {},
   "source": [
    "## Step 8) Run the statistical sampler!! \n",
    "\n",
    "Now we can finally move forward with running ultranest. Though once you have gotten this far you will have the skills to implement other samplers as well. They all generally have the same format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62618272-3fd6-4a89-9ea6-c7407c3951f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f12b7d26-63e2-487a-b22d-1c956cad6123",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ultranest.ReactiveNestedSampler(PARAMS, loglikelihood, PRIOR)\n",
    "result = sampler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11c92ae3-78dd-4990-a98f-f605eed5afb7",
   "metadata": {},
   "source": [
    "# The Basics of Retrieval Analysis\n",
    "\n",
    "In future tutorials you will see some pre-defined functions to help you with doing the following analyses. However, on a first go-around in this simple example we will put the native code here so you can see what is going on under the hood. \n",
    "\n",
    "## Posterior predictive checks\n",
    "\n",
    "First let us check that our samples are returning a sensible model for our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1afb99-9b6c-4005-87ba-2c0e86a16da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultranest.plot import PredictionBand\n",
    "\n",
    "plt.figure()\n",
    "first =True\n",
    "for params in result['samples']:\n",
    "    x,y,off,err = MODEL(params)\n",
    "    if first:\n",
    "        band = PredictionBand(1e4/x);first=False   #transforming xaxis to microns \n",
    "    band.add(y)\n",
    "\n",
    "band.line(color='g')#median model\n",
    "\n",
    "#lets plot the 1, 2, and 3 sigma confidence interval from these samples \n",
    "for q ,key in zip([k/100/2 for k in [68.27, 95.45, 99.73]], ['1sig','2sig','3sig']): \n",
    "        band.shade(q=q,color='g', alpha=0.5)\n",
    "\n",
    "for ikey in DATA_DICT.keys(): \n",
    "    plt.errorbar(x=1e4/DATA_DICT[ikey][0], y=DATA_DICT[ikey][1], yerr=DATA_DICT[ikey][2], marker='o', ls=' ',label='Grant data')\n",
    "    \n",
    "plt.xlabel('micron')\n",
    "plt.ylabel('transit depth');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650b8cd1-5c1f-4110-91e0-4bc665e078d1",
   "metadata": {},
   "source": [
    "Not bad! We are nearly ready to implement a real atmosphere model to fit the data! \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad57beaa-8dba-4100-84c7-ee8689ef8f85",
   "metadata": {},
   "source": [
    "## Parameter posterior probability distribution checks\n",
    "\n",
    "What to check for in a corner plot: \n",
    "\n",
    "1. Are your 1D marginalized posterior probability distributions Gaussian (bell shaped)?\n",
    "2. Are there correlations between your parameters?\n",
    "3. Does your posterior probability distribution hit the limit of your prior (check the x axis of each distribution)?\n",
    "\n",
    "For our example below: m and b appear Gaussian. However, the error inflation term looks to be poorly constrained. We only have an upper limit on the number. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a83843b-61df-44ee-b583-59cb6ac09cca",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultranest.plot import cornerplot\n",
    "cornerplot(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25cbf3f5-5d41-48bd-911f-0cd9b5e54067",
   "metadata": {},
   "source": [
    "## Exercise to check your understanding \n",
    "\n",
    "1. Go back through param_set, prior_set and try rerunning the retrieval that does not include the error inflation term. \n",
    "\n",
    "TIP: If you follow the formalism you will not have to change the likelihood code. You should only have to make edits to the param_set and  prior_set (and guesses_set if you are using this to check your model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a1dc28-2eea-4ace-9638-0c8684d8b6a5",
   "metadata": {},
   "source": [
    "# Short cut to get grid fitting retrieval template in script form \n",
    "\n",
    "Now that you understand the basics of running a simple line model let's introduce some PICASO tools to help you quickly setup a new retrieval script, without the hassle of a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed83be7d-27a9-470b-b888-89e3109163f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import picaso.retrieval as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e59d26-ba54-430f-b80d-8fde0eb4d1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtype='line' #first lets specify the retrieval type 'line' (we will introduce the other options in future tutorials)\n",
    "sript_name='run_test.py' #speciy a script name \n",
    "sampler_output = '/data/test/ultranest/line' #what folder do you want your ultranest output to go to? \n",
    "pr.create_template(rtype,sript_name,sampler_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8843afa6-3e72-487a-8d68-77a1a43ede6f",
   "metadata": {},
   "source": [
    "Open up `run_test.py` and modify what you need. We have marked key areas you might want to modify with \"CHANGEME\"\n",
    "\n",
    "Running with mpiexec with 5 cpu: \n",
    "\n",
    "    >> mpiexec -n 5 python -m mpi4py run_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8346e31d-c7f5-4eb1-9aa3-246aeae813bb",
   "metadata": {},
   "source": [
    "## Further Reading\n",
    "\n",
    "https://johannesbuchner.github.io/UltraNest/example-sine-bayesian-workflow.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526ea1c-2187-4549-91a7-2f7f968fd850",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
