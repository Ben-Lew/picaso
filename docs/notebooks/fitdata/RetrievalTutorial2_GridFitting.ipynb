{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f6da004-2fb1-47fe-b934-0e2ed17c2369",
   "metadata": {},
   "source": [
    "# Implemenation: Grid Fitting using Bayesian Statistics w/ PICASO\n",
    "\n",
    "If this is your first crack at fitting parameters with PICASO, we strong encourage you to look at the first retrieval tutorial. Here we will expand on that tutorial and you will learn: \n",
    "\n",
    "**What you will learn:** \n",
    "\n",
    "1. How to load a grid and fit parameters using pre-computed spectra\n",
    "2. How to analyze grid fitting results\n",
    "\n",
    "**What you should know:**\n",
    "\n",
    "1. `picaso`'s retrieval class structure (model_set, prior_set, param_set)\n",
    "2. `picaso.retrieval.setup_retrieval` function to generate template scripts (covered in Retrieval Tutorial 1)\n",
    "\n",
    "**What you will need:** \n",
    "\n",
    "We will build on the `line` fitting tutorial 1, using Grant et al. 2024 data. \n",
    "\n",
    "1. Download [WASP-17b grids here](www.doi.org/10.5281/zenodo.14681144) (and unpacked it is 436 Mb)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4669e156-5b15-47ca-b25a-33b4a9385397",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os \n",
    "import picaso.justdoit as jdi\n",
    "import picaso.analyze as lyz\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7a1039-5c45-4951-94c7-b07043ebbb71",
   "metadata": {},
   "source": [
    "## Step 1) Develop function to get data\n",
    "\n",
    "(same as tutorial 1) \n",
    "\n",
    "Let's create a function to pull our data where all we need to do is declare who the data is from and it pulls it for us automatically.\n",
    "\n",
    "Note: this format is only a recommendation and you can change any part of this to fit your needs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca69d4b-d302-453c-acbc-7e0c930dc8d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(): \n",
    "    \"\"\"\n",
    "    Create a function to process your data in any way you see fit.\n",
    "    Here we are using the ExoTiC-MIRI data \n",
    "    https://zenodo.org/records/8360121/files/ExoTiC-MIRI.zip?download=1\n",
    "    But no need to download it.\n",
    "\n",
    "    Checklist\n",
    "    ---------\n",
    "    - your function returns a spectrum that will be in the same units as your picaso model (e.g. rp/rs^2, erg/s/cm/cm or other) \n",
    "    - your function retuns a spectrum that is in ascending order of wavenumber \n",
    "    - your function returns a dictionary where the key specifies the instrument name (in the event there are multiple)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict: \n",
    "        dictionary key: wavenumber (ascneding), flux or transit depth, and error.\n",
    "        e.g. {'MIRI LRS':[wavenumber, transit depth, transit depth error], 'NIRSpec G395H':[wavenumber, transit depth, transit depth error]}\n",
    "    \"\"\"\n",
    "    dat = xr.load_dataset(jdi.w17_data())\n",
    "    #build nice dataframe so we can easily \n",
    "    final = jdi.pd.DataFrame(dict(wlgrid_center=dat.coords['central_wavelength'].values,\n",
    "                transit_depth=dat.data_vars['transit_depth'].values,\n",
    "                transit_depth_error=dat.data_vars['transit_depth_error'].values))\n",
    "\n",
    "    #create a wavenumber grid \n",
    "    final['wavenumber'] = 1e4/final['wlgrid_center']\n",
    "\n",
    "    #always ensure we are ordered correctly\n",
    "    final = final.sort_values(by='wavenumber').reset_index(drop=True)\n",
    "\n",
    "    #return a nice dictionary with the info we need \n",
    "    returns = {'MIRI_LRS': [final['wavenumber'].values, \n",
    "             final['transit_depth'].values  ,final['transit_depth_error'].values]   }\n",
    "    return returns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fa11547-dd36-4add-92a1-f387fbb98244",
   "metadata": {},
   "source": [
    "## Step 2) Load Grid "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bd42e4-150f-443d-aee1-6ef49a1cfac7",
   "metadata": {},
   "source": [
    "Let's point towards the grid locations, create a grid fitter object for them, and prep them. \n",
    "\n",
    "If you are not familiar with `lyz.GridFitter` we encourate you to first become familiar with non-Bayesian grid fitting based on purely maximum chi-sq values. You can play around with this [Grid Search tutorial here](https://natashabatalha.github.io/picaso/notebooks/fitdata/GridSearch.html). \n",
    "\n",
    "The basic premise of `prep_gridtrieval`:\n",
    "- vets and transforms the grid to ensure it's square and interprelate-able\n",
    "- checks that there is a common pressure grid for the temperature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9d396a5-51f6-476c-ac6e-293ac28327b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_location = '/data2/models/WASP-17b/spec/zenodo/v1'# should ultimately point to location of all .nc files\n",
    "grid_name = 'cldfree' #for your own book-keeping\n",
    "fitter = lyz.GridFitter(grid_name,grid_location, verbose=True, to_fit='transit_depth', save_chem=True)\n",
    "fitter.prep_gridtrieval(grid_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915cef03-9462-49ee-95e1-4064790192a6",
   "metadata": {},
   "source": [
    "## Step 3) Param Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e13e3cd-6ca3-4bf5-841c-ecba467f5a77",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can get the grid parameters directly from the module load, this will help with setting up our free parameters\n",
    "grid_parameters_unique = fitter.interp_params[grid_name]['grid_parameters_unique']\n",
    "\n",
    "class param_set:\n",
    "    \"\"\"\n",
    "    This is much easier now since it is the grid parameters plus an offset to account for unknown reference pressure\n",
    "    \"\"\"\n",
    "    grid = list(grid_parameters_unique.keys())+['offset']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5973b3b4-5320-406b-9581-562dcc5cacb1",
   "metadata": {},
   "source": [
    "## Step 4) Guesses Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613f686b-5bd2-4847-a5b6-581c007afeca",
   "metadata": {},
   "source": [
    "In testing, it is very useful to check that it is grabbing the right parameters before doing a full analysis. This is available  for a sanity check if desired"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a881c5-15e8-4f9d-a1fe-5881d002501b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class guesses_set: \n",
    "    \"\"\"\n",
    "    For our guesses, we can verify things are working by just taking the first instance of each grid point\n",
    "    \"\"\"\n",
    "    #completely random guesses just to make sure it runs\n",
    "    grid=[grid_parameters_unique[i][0]\n",
    "             for i in grid_parameters_unique.keys()] + [0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3597fc70-bdfe-49fc-bc4f-2c07f9c5644c",
   "metadata": {},
   "source": [
    "## Step 5) Model Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e52d36dd-ad98-44f9-9354-5e18ccf9f42b",
   "metadata": {},
   "source": [
    "Let's set up our grid interpolator using picaso's custom interp "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dee0214-5a97-4ec7-a3ec-2ba95f333f12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class model_set:\n",
    "    \"\"\"\n",
    "    There are several different ways to interpolate onto a grid. Here, we use picaso's fast custom interpolator that \n",
    "    uses a flex linear interpolator and can be used on any type of matrix grid. \n",
    "    \"\"\"     \n",
    "    def grid(cube): \n",
    "        \"\"\"Simple grid interpolation + offset for spectra\n",
    "        \"\"\"\n",
    "        final_goal = cube[0:len(grid_parameters_unique.keys())]\n",
    "        spectra_interp = lyz.custom_interp(final_goal, fitter, grid_name)\n",
    "        micron = fitter.wavelength[grid_name]\n",
    "        wno = 1e4/fitter.wavelength[grid_name] \n",
    "        offset={'MIRI_LRS':cube[-1]} #remember before it helps always to have the same model returns\n",
    "        error_inf={} # let's not add error inf \n",
    "        return wno, spectra_interp,offset,error_inf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecb4e125-135f-4bfd-a60f-a744febc8c2d",
   "metadata": {},
   "source": [
    "## Step 6) Prior Set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f80e3ae0-e80e-4e34-acea-0a58fa6263f6",
   "metadata": {},
   "source": [
    "Finally, we are storing all the priors for Ultranest to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74043db2-4dec-4622-bded-86d1aa36502e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class prior_set:\n",
    "    \"\"\"\n",
    "    Store all your priors. You should have the same exact function names in here as\n",
    "    you do in model_set and param_set\n",
    "\n",
    "    Make sure the order of the unpacked cube follows the unpacking in your model \n",
    "    set and in your parameter set. \n",
    "    #pymultinest: http://johannesbuchner.github.io/pymultinest-tutorial/example1.html\n",
    "    \"\"\"   \n",
    "    def grid(cube):\n",
    "        params = cube.copy()\n",
    "        for i,key in enumerate(grid_parameters_unique): \n",
    "            minn = np.min(grid_parameters_unique[key]) \n",
    "            maxx = np.max(grid_parameters_unique[key]) \n",
    "            params[i] = minn + (maxx-minn)*params[i]\n",
    "        #the offset parameter here let's allow these spectra to move up and down by ~1000pm. This is a guess so let's check firts! \n",
    "        minn=.008;maxx=.015;\n",
    "        i+=1;params[i] = minn + (maxx-minn)*params[i]\n",
    "        return params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7390abd-8600-427d-9bb3-b6ace0daaaa3",
   "metadata": {},
   "source": [
    "## Step 7) Loglikelihood\n",
    "\n",
    "No changes from our simple line example. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7f656a7-b093-4ab6-b1da-86515b5ec604",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loglikelihood(cube):\n",
    "    \"\"\"\n",
    "    Log_likelihood function that ultimately is given to the sampler\n",
    "    Note if you keep to our same formats you will not have to change this code move \n",
    "\n",
    "    Tips\n",
    "    ----\n",
    "    - Remember how we put our data dict, error inflation, and offsets all in dictionary format? Now we can utilize that \n",
    "    functionality if we properly named them all with the right keys! \n",
    "\n",
    "    Checklist\n",
    "    --------- \n",
    "    - ensure that error inflation and offsets are incorporated in the way that suits your problem \n",
    "    - note there are many different ways to incorporate error inflation! this is just one example \n",
    "    \"\"\"\n",
    "    #compute model spectra\n",
    "    resultx,resulty,offset_all,err_inf_all = MODEL(cube) # we will define MODEL below \n",
    "\n",
    "    #initiate the four terms we willn eed for the likelihood\n",
    "    ydat_all=[];ymod_all=[];sigma_all=[];extra_term_all=[];\n",
    "\n",
    "    #loop through data (if multiple instruments, add offsets if present, add err inflation if present)\n",
    "    for ikey in DATA_DICT.keys(): #we will also define DATA_DICT below\n",
    "        xdata,ydata,edata = DATA_DICT[ikey]\n",
    "        xbin_model , y_model = jdi.mean_regrid(resultx, resulty, newx=xdata)#remember we put everything already sorted on wavenumber\n",
    "\n",
    "        #add offsets if they exist to the data\n",
    "        offset = offset_all.get(ikey,0) #if offset for that instrument doesnt exist, return 0\n",
    "        ydata = ydata+offset\n",
    "\n",
    "        #add error inflation if they exist\n",
    "        err_inf = err_inf_all.get(ikey,0) #if err inf term for that instrument doesnt exist, return 0\n",
    "        sigma = edata**2 + (err_inf)**2 #there are multiple ways to do this, here just adding in an extra noise term\n",
    "        if err_inf !=0: \n",
    "            #see formalism here for example https://emcee.readthedocs.io/en/stable/tutorials/line/#maximum-likelihood-estimation\n",
    "            extra_term = np.log(2*np.pi*sigma)\n",
    "        else: \n",
    "            extra_term=sigma*0\n",
    "\n",
    "        ydat_all.append(ydata);ymod_all.append(y_model);sigma_all.append(sigma);extra_term_all.append(extra_term); \n",
    "\n",
    "    ymod_all = np.concatenate(ymod_all)    \n",
    "    ydat_all = np.concatenate(ydat_all)    \n",
    "    sigma_all = np.concatenate(sigma_all)  \n",
    "    extra_term_all = np.concatenate(extra_term_all)\n",
    "\n",
    "    #compute likelihood\n",
    "    loglike = -0.5*np.sum((ydat_all-ymod_all)**2/sigma_all + extra_term_all)\n",
    "    return loglike"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae852941-bbd3-46dd-bf06-42c13aae758f",
   "metadata": {},
   "source": [
    "## Step 7) Check models, likelihoods, priors! \n",
    "\n",
    "It looks like our priors are giving us an offset that is evenly distributed about the data. Looks good! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35859d99-e4e9-4d9f-8827-b937202d4885",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can easity grab all the important pieces now that they are neatly stored in a class structure \n",
    "DATA_DICT = get_data()\n",
    "PARAMS = getattr(param_set,'grid')\n",
    "MODEL = getattr(model_set,'grid')\n",
    "PRIOR = getattr(prior_set,'grid')\n",
    "GUESS = getattr(guesses_set,'grid')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e3432ba-b417-45c5-8456-d94a306eeb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.figure()\n",
    "#lets plot the data \n",
    "for ikey in DATA_DICT.keys(): \n",
    "    plt.errorbar(x=DATA_DICT[ikey][0], y=DATA_DICT[ikey][1], yerr=DATA_DICT[ikey][2],color='black',\n",
    "                 marker='o', ls=' ',label='Grant data')\n",
    "\n",
    "ntests = 10 #lets do 10 random tests \n",
    "for i in range(ntests): \n",
    "    cube = np.random.uniform(size=len(PARAMS))\n",
    "    params_evaluations = PRIOR(cube)\n",
    "    x,y,off,err = MODEL(params_evaluations)\n",
    "    loglike = loglikelihood(params_evaluations)\n",
    "    #REMEMBER: we are adding an offset parameter in the LIKELIHOOD \n",
    "    #so when you check you have to make sure that your \n",
    "    #offset is included below \n",
    "    plt.plot(x,y-params_evaluations[-1],label=str(i)+str(int(loglike)))\n",
    "\n",
    "guessx,guessy,off,err = MODEL(GUESS)\n",
    "guess_log = loglikelihood(GUESS)\n",
    "plt.plot(guessx,guessy-GUESS[-1],color='black',label='guess '+ str(int(guess_log)))\n",
    "plt.xlim([1e4/14,1e4/5])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271cc4f5-bbe6-488b-a7da-de6e5d822eeb",
   "metadata": {},
   "source": [
    "The models that are chosen look randomly distributed around the data. Looks like there are no issues with our sampler. Let's go ahead and run ultranest. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92a80fd8-8d82-4d87-abb6-a98b24a5ae5e",
   "metadata": {},
   "source": [
    "## Step 8) Run the statistical sampler!! \n",
    "\n",
    "This will take a little longer than the simple line since we have more parameters now. But the basics are still identical. \n",
    "\n",
    "Some things to notice while the sampler is running: \n",
    "\n",
    "1. The ultranest visualization (purple) will show you where the solution is headed towards. It looks like (for example) M/H is immediately drawn toward higher values.\n",
    "2. For parameters like the offset, where we have manually set the prior, make sure that you are seeing a solution which is not going off the bounds of the prior. In this case, it looks like it is converging right at the median of those two values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab804d2-806b-4f4e-87f1-5d7872e4970b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ultranest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6a294-5074-4e10-9f61-5c474e5ce7f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "sampler = ultranest.ReactiveNestedSampler(PARAMS, loglikelihood, PRIOR,resume=True,log_dir='/data/test/ultranest/grid')\n",
    "#note if you wanted to turn thsi in the notebook and save the output you would add resume and log_dir above to save\n",
    "result = sampler.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b51335-49a7-4688-a756-34492c602d56",
   "metadata": {},
   "source": [
    "## Analyze Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c275d3ac-d301-429e-b9a0-bd9879f9c22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultranest.plot import PredictionBand\n",
    "\n",
    "plt.figure()\n",
    "first =True\n",
    "for params in result['samples']:\n",
    "    x,y,off,err = MODEL(params)\n",
    "    if first:\n",
    "        band = PredictionBand(1e4/x);first=False   #transforming xaxis to microns \n",
    "    \n",
    "    #this is a little tricky as we have added an offset to the data \n",
    "    #so what ill do in this case is add the inverse to the model so we can visualise it with the data in the banded solution\n",
    "    band.add(y-params[-1])\n",
    "\n",
    "band.line(color='black')#median model\n",
    "\n",
    "#lets plot the 1, 2, and 3 sigma confidence interval from these samples \n",
    "for q ,key in zip([k/100/2 for k in [68.27, 95.45, 99.73]], ['1sig','2sig','3sig']): \n",
    "        band.shade(q=q,color='grey', alpha=0.2)\n",
    "\n",
    "for ikey in DATA_DICT.keys(): \n",
    "    plt.errorbar(x=1e4/DATA_DICT[ikey][0], y=DATA_DICT[ikey][1], yerr=DATA_DICT[ikey][2], color='red',marker='o', ls=' ',label='Grant data')\n",
    "    \n",
    "plt.xlabel('micron')\n",
    "plt.ylabel('transit depth');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6228a29-745c-43e7-b470-3e41bb2a700f",
   "metadata": {},
   "source": [
    "# Short cut to get grid fitting retrieval template in script form\n",
    "\n",
    "Same as before but now we will add `grid_kwargs` to set some additional features in the `GridFitter`. Note this is only optional and you are always free to change the template manually. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507eb27b-3baf-4a11-ac21-fc57f45bdf6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import picaso.retrieval as pr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5754379-1eed-484b-bb88-191d9e74f732",
   "metadata": {},
   "outputs": [],
   "source": [
    "rtype='grid' #first lets specify the retrieval type 'grid' \n",
    "sript_name='run_test.py' #speciy a script name \n",
    "sampler_output='/data/test/ultranest/grid'\n",
    "\n",
    "grid_location = '/data2/models/WASP-17b/spec/zenodo/v1'# should ultimately point to location of all .nc files\n",
    "grid_name = 'cldfree' #for your own book-keeping\n",
    "to_fit = 'transit_depth' #this is based on what you want to fit in the xarray files s\n",
    "\n",
    "#new!\n",
    "grid_kwargs={'grid_location':grid_location,'grid_name':grid_name,'to_fit':to_fit}\n",
    "\n",
    "pr.create_template(rtype,sript_name,sampler_output,grid_kwargs=grid_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5661bce8-b66f-4885-904e-a9cb41759a55",
   "metadata": {},
   "source": [
    "Open up `run_test.py` and modify what you need. We have marked key areas you might want to modify with \"CHANGEME\"\n",
    "\n",
    "Running with mpiexec with 5 cpu: \n",
    "\n",
    "    >> mpiexec -n 5 python -m mpi4py run_test.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8801ee4b-277c-4d0e-a389-26acdc1c0a58",
   "metadata": {},
   "source": [
    "## Some tricks and tips for once you have a built a script\n",
    "\n",
    "Now that you have a built script we can input it to grab our built models, priors, parameters set, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9e4d558-3b42-4d2a-b58a-a3e6fcfb518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import run_test as rt #YOUR OWN unique script "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa8e5e6-6e63-4b83-8ade-24e7fb2f15d0",
   "metadata": {},
   "source": [
    "Now you can easily grab what you need directly and no need to have this in a notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88b75985-ee54-49eb-abf8-b019e9021af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DICT = rt.get_data()\n",
    "PARAMS = getattr(rt.param_set,'grid')\n",
    "MODEL = getattr(rt.model_set,'grid')\n",
    "PRIOR = getattr(rt.prior_set,'grid')\n",
    "GUESS = getattr(rt.guesses_set,'grid')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db463520-3406-4abf-b9fe-cb04af7f91b9",
   "metadata": {},
   "source": [
    "We can also see use the fitter class to continue our analysis (which we will do in the following section)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0de35571-944a-496f-ba0f-7ded89c85c12",
   "metadata": {},
   "source": [
    "# PICASO Analysis Tools from Saved Samples \n",
    "\n",
    "This part relies on having ran the above script for the run_test.py and having output in a directory defined by `sampler_output`\n",
    "## Auto read ultranest results\n",
    "\n",
    "This function gives you back some of the most highly used output products: \n",
    "\n",
    "- `samples_equal`: equally weighted samples from the posterior\n",
    "- `max_logl` : the maximum loglikelihood (otherwise known as the Bayesian evidence)\n",
    "- `max_logl_point` : the set of parameters that is associated with the maximum loglikelihood\n",
    "- `med_intervals` : the 1 sigma median constraint intervals for each of the parameters of interest\n",
    "- `ultranest_out` : the raw ultranest output "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd2b02b-211c-496a-ad7d-1c1bf2bf39c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pr.get_info(sampler_output, PARAMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fb6d607-2676-49e3-a09a-347924c7cca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "results.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3108cf10-9a0f-462e-aca8-7c52c4935ce8",
   "metadata": {},
   "source": [
    "## Get spectra interval bands and evaluate max loglikelihood spectra  \n",
    "\n",
    "- `bands_spectra`: 1,2,3 sigma bands for spectra (contains keys such as `1sig_lo` and `1sig_hi` for the spectra)\n",
    "- `max_logl_spectra`: the spectra evaluated at the max logl point\n",
    "- `max_logl_error_inflation` : if exists, the error inflation associated with the max logl point\n",
    "- `max_logl_offsets` : If exists, the offsets associated with the max logl point\n",
    "- `wavelength`: the wavelength grid in um "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0478ca48-386a-4f7e-ad1b-1767b08b37d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_draws=200 #number of evaluations for which to \n",
    "evaluations = pr.get_evaluations(results['samples_equal'], results['max_logl_point'], MODEL, n_draws,\n",
    "                                 regrid=100.0,#spectral resolution to regrid to\n",
    "                                 pressure_bands=[])#'temperature','H2O','CO2','CO','CH4']) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46444175-63e6-49ea-8f68-ebe94b7e1df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluations.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c8de9d2-bbe0-420e-8606-868e0eb34080",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(7,5))\n",
    "color_scale =  pr.pals.Blues[3]\n",
    "resolution=100\n",
    "f=pr.plot_spectra_bands(evaluations,color_scale, ax=axs,R=resolution)\n",
    "axs.set_title('Cld Free Model')#add other styles here \n",
    "axs.set_xlim([5,14])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7132c3b0-29e2-4ba5-9238-922d81e2875d",
   "metadata": {},
   "source": [
    "## Get Reduced Chi Squared Statistic of Max Logl spectrum Incl Offsets\n",
    "\n",
    "- `wavenumber`: new regridded wavenumber grid on data axis\n",
    "- `model` : model regridded on data axis\n",
    "- `datay` : data with offset included\n",
    "- `datae` : data error (no error inflation included )\n",
    "- `chisq_per_datapt` : chi squared per data point (DOF=len of data array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7ec0394-b513-4403-bf9e-9a0a059a3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_info = pr.get_chisq_max(evaluations, DATA_DICT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddb0d911-f961-4738-b494-23d76e6ee720",
   "metadata": {},
   "outputs": [],
   "source": [
    "chi_info.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03cb1d37-ce89-4a89-a060-d719f65ceb25",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(1e4/chi_info['wavenumber'], chi_info['model'],color='red',label='max logl model')\n",
    "plt.errorbar(x=1e4/chi_info['wavenumber'], y=chi_info['datay'], yerr=chi_info['datae'], \n",
    "             color='black',marker='o', ls=' ')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76b0771-a63d-4d3e-91b0-893895ae6e87",
   "metadata": {},
   "source": [
    "## Get all bundled results in xarray format\n",
    "\n",
    "What does `data_output` do? \n",
    "\n",
    "1. bundles all the median and sigma banded output data into an xarray\n",
    "2. adds all the constraint intervals to your xarray in latex format\n",
    "3. creates some default plots of banded spectra (banded chem if you have created it) and corner plots\n",
    "4. creates a pickle of your equally weighted samples if you have requested it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2af54183-c8c2-417a-b58d-1a4d4e842f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = '/data/test/output/tagname'#name of file (without extension!!) where you want the output returned \n",
    "return_samples=True # do you want a pickle file created of all the equally weighted samples? \n",
    "spectrum_tag = 'transit_depth'#in the xarray what do you want the spectrum called? \n",
    "spectrum_unit= 'cm**2/cm**2'#what are the units of your spectrum? \n",
    "author = \"NE Batalha\"#who did the analysis? this is for the xarray\n",
    "contact=\"natasha.e.batalha@nasa.gov\"#how can you be contacted? this is for the xarray\n",
    "model_description=\"cloud free grid fit\"#describe your model so people will know what it is \n",
    "code = \"PICASO,Ultranest\" # what codes did you use for this analysis? \n",
    "\n",
    "bxr=pr.data_output(evaluations, results, chi_info, filename,return_samples=True,\n",
    "                     spectrum_tag=spectrum_tag,spectrum_unit=spectrum_unit,\n",
    "                    author=author,contact=contact,\n",
    "                    model_description=model_description,\n",
    "                    code=code,)#,\n",
    "                    #round=[2, 1,1,5,3,2,2,1,2,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12687fd-da76-4abc-82aa-d00359dac0c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bxr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "894b5617-0b9f-4c44-8796-65b51dbb7fab",
   "metadata": {},
   "outputs": [],
   "source": [
    "bxr.attrs['intervals_params']\n",
    "#note if you do not like the round errors here you can adjust what these numbers are rounded to with \n",
    "#round kwarg to data_output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54a593ee-379b-4575-a81a-1ab49e065929",
   "metadata": {},
   "source": [
    "## Stylize Plots\n",
    "\n",
    "Our default plots were not great, let's beautify them a little "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a26b839-6826-4650-8571-f6676ddaec6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axs = plt.subplots(1,1,figsize=(7,5))\n",
    "color_scale =  pr.pals.Blues[3]\n",
    "resolution=100\n",
    "f=pr.plot_spectra_bands(evaluations,color_scale, ax=axs,R=resolution)\n",
    "axs.set_title('Cld Free Model')#add other styles here \n",
    "axs.set_xlim([5,14])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a8d935-e818-4ebb-a321-95e8d66d0f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = results['samples_equal']\n",
    "\n",
    "params =  results['param_names']\n",
    "\n",
    "#create mapper for labels \n",
    "pretty_labels={'cto':'C/O',\n",
    "               'mh':'log M/H [xSolar]',\n",
    "               'heat_redis':'heat redis.',\n",
    "               'offset':'O$_\\mathrm{LRS}$',\n",
    "               'tint':r'T$_\\mathrm{int}$'}\n",
    "\n",
    "#create mapper for ranges ?\n",
    "ranges={'cto':[0.25,1],\n",
    "        'tint':[200,300],\n",
    "        'offset':[0.0085,0.0095],\n",
    "        'heat_redis':[0.5,0.9],\n",
    "        'mh':[1.3,1.7]}\n",
    "\n",
    "ints = eval(bxr.attrs['intervals_params'])#get pretty titles \n",
    "intervals={i:ints[i] for i in results['param_names']}\n",
    "\n",
    "f,a=pr.plot_pair(samples,params,pretty_labels=pretty_labels, ranges=ranges,figsize=(15,15),\n",
    "              intervals=intervals)\n",
    "#could make additional style change using a here \n",
    "f.savefig('/data/test/output/plot_pair.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "224a9f02-a337-4e65-809a-57a9749645ee",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
